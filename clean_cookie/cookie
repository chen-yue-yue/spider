import scrapy
from scrapy_lianjia.items import ScrapyLianjiaItem

class XinlongchengSpider(scrapy.Spider):
    name = "xinlongcheng"
    allowed_domains = ["bj.lianjia.com"]
    start_urls = ["https://bj.lianjia.com/ershoufang/c1111027381003/"]

    async def start(self):
        headers={  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                    "Accept-Language": "zh-CN,zh;q=0.9",
                    "Referer": "https://www.lianjia.com/",  # 使用正确的Referer
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Connection": "keep-alive",
                    "Cookie":"SECKEY_ABVK=vvR+KBylzs341tIeurubnC63QkuzoCT4z28NwsewLu8%3D; BMAP_SECKEY=HONR2i8zc6cMsGE7x3f74kMeK0YTAPBcnmTv7Gp0Ro5gleGJusap5tyBcSMGE-obRC3mH_rO9q1uFNX8o4vniGg3BXqwWSPWmYy9xWEYEDzh6xCMRN3c8ns-7UMcyFVGUsaVrpKtg-4nCa82C6KUWkAoFKVLIrd5G-_8ZybetM9-OhdOQsFf56b3SVbb_Ad9; lianjia_uuid=d929ba72-679f-4718-8933-4901ec4ff338; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2219b0c307531211-0ff3e212874-26061b51-1821369-19b0c30753222cd%22%2C%22%24device_id%22%3A%2219b0c307531211-0ff3e212874-26061b51-1821369-19b0c30753222cd%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%7D; crosSdkDT2019DeviceId=-t4vocm--1vlpj0-oiejge3gamr0gu6-h87tudo0r; _ga=GA1.2.1772040068.1765436072; _gid=GA1.2.1003414472.1765436072; ftkrc_=58a7b8d4-022b-42ee-90c9-4bcdb435ca79; lfrc_=5a67ae8f-1cf4-4aa8-96c3-4b86f1901c30; select_city=110000; _jzqckmp=1; Hm_lvt_46bf127ac9b856df503ec2dbf942b67e=1765436061,1765456879,1765514166,1765589539; HMACCOUNT=3F18AC6E8ACD690D; _qzjc=1; _jzqc=1; lianjia_ssid=15efd7ef-3765-42e3-a465-6d95b0200a53; login_ucid=2000000483328062; lianjia_token=2.0012f6530b47bc3ed9035b7a3a3e971a69; lianjia_token_secure=2.0012f6530b47bc3ed9035b7a3a3e971a69; security_ticket=IeBuEYZ747wMIWqW31xzPZqZBcTgouskFZujcwMRrieoCMOitoS7JbKlPWhUFj5WKuyueFBXA3wJGocJaaq12I4WzdmU5kN9CN0YolrktKv+adjpeAhGPsUGwhZekxIfSXirVYnl+z5bxPUArz7G25i+owzAMZ/cvnDvvADKKVU=; _qzja=1.1379139782.1765436061258.1765591577117.1765601152811.1765594983850.1765601152811.0.0.0.58.10; _qzjto=8.3.0; _jzqa=1.787719326959594200.1765436061.1765591577.1765601153.10; _jzqx=1.1765453672.1765601153.4.jzqsr=bj%2Elianjia%2Ecom|jzqct=/.jzqsr=clogin%2Elianjia%2Ecom|jzqct=/; Hm_lpvt_46bf127ac9b856df503ec2dbf942b67e=1765601153; srcid=eyJ0Ijoie1wiZGF0YVwiOlwiZjg1ZjllNDhjZjM2ZDNjZTE0ZjI1OTVjNmNhNjM0ZDFmNTZhNGJkMWY1OGQ5MDdmMmFjMjBkYWRhOWRhZTg1Njg0NGMyNzBjOGU1NTIwYTcyM2VlNjMwYmE5OWY1NDA1NTY1ZGVkOGY0MDI3MmI5MTlmZjY0YjA3YzVmNzEyMWQ2ZmE1MDljNjNjMWE5OTIwNmNkNWM4MmUwMzFjNGNjZTQ0NWQ3OGI4OTRlMTE0MDkyYjZiNzc1NGUwNWNlMzFmYzlkZDM2MWQxYmIyZmZiMjA4ZjdhNWFlMjc2NDVjMzViZDU5NzQ4NTlkM2MyOGE5ODk4YmY1ZDA1MzM2ZGQ0Y1wiLFwia2V5X2lkXCI6XCIxXCIsXCJzaWduXCI6XCJmNzViYTBiZFwifSIsInIiOiJodHRwczovL2JqLmxpYW5qaWEuY29tL2Vyc2hvdWZhbmcvYzExMTEwMjczODEwMDMvIiwib3MiOiJ3ZWIiLCJ2IjoiMC4xIn0=; _qzjb=1.1765601152811.1.0.0.0; _jzqb=1.1.10.1765601153.1; _gat=1; _gat_past=1; _gat_global=1; _gat_new_global=1; _gat_dianpu_agent=1; _ga_KJTRWRHDL1=GS2.2.s1765601164$o7$g0$t1765601164$j60$l0$h0; _ga_QJN1VP0CMS=GS2.2.s1765601164$o7$g0$t1765601164$j60$l0$h0"}
        for url in self.start_urls:
            yield scrapy.Request(
                url=url,
                method ="GET",
                callback=self.parse,
                headers=headers,
                meta={"download_timeout": 15, "max_retry_times": 3},
                # 允许重定向到登录域名
                dont_filter=True
            )

    def parse(self, response):
        try:
            info_list=response.xpath("//div[@class='info clear']")
            for info in info_list:
                #标题
                title = info.xpath("./div[@class = 'title']/a/text()").get()
                title= title.strip() if title else ""
                if not title:
                    self.logger.warning(f"房源标题解析失败：{info}")
                #标签
                clean_tags = []
                tags = info.xpath("./div[@class = 'tag']//text()").getall()
                for tag in tags:
                    stripped_tag = tag.strip()
                    if stripped_tag:
                        clean_tags.append(stripped_tag)
                tag = ",".join(clean_tags)
                if not clean_tags:
                    self.logger.warning("房源标签解析失败（无有效标签）")
                #总价
                clean_price=[]
                totalprice_list = info.xpath(".//div[@class = 'totalPrice totalPrice2']//text()").getall()
                for p in totalprice_list:
                    stripped_price = p.strip()
                    if stripped_price:
                        clean_price.append(stripped_price)
                        total_price = "".join(clean_price)
                if not clean_price:
                    self.logger.warning("房源总价解析失败")
                    total_price = "未知总价"
                #单价
                unitprice = info.xpath(".//div[contains(@class,'unitPrice')]/text()").get()
                unitprice = unitprice.strip() if unitprice else ""
                if not unitprice:
                    self.logger.warning("房源单价解析失败")
                    unitprice = "未知单价"
                #位置
                clean_position=[]
                position_list = info.xpath(".//div[@class='positionInfo']//text()").getall()
                for po in position_list:
                    stripped_position = po.strip()
                    if stripped_position:
                        clean_position.append(stripped_position)
                        position= "".join(clean_position)
                if not clean_position:
                    self.logger.warning("房源位置解析失败")
                    position = "未知位置"
                #房屋格局（几室几厅）房屋面积、朝向、装修状态、楼层、楼栋类型（板楼/塔楼等）
                details = info.xpath(".//div[@class ='houseInfo']/text()").get()
                details_list=details.split('|')
                house_range = details_list[0].strip()  # 房屋格局
                house_area = details_list[1].strip()   # 房屋面积
                house_toward = details_list[2].strip() # 朝向
                house_situation = details_list[3].strip() # 装修状态
                house_floor = details_list[4].strip()  # 楼层
                house_year = details_list[5].strip()   # 年份
                house_type = details_list[6].strip()   # 楼栋类型

                # 创建Item并填充数据
                item = ScrapyLianjiaItem()
                item['title'] = title
                item['tag'] = tag
                item['total_price'] = total_price
                item['unitprice'] = unitprice
                item['position'] = position
                item['house_range'] = house_range
                item['house_area'] = house_area
                item['house_toward'] = house_toward
                item['house_situation'] = house_situation
                item['house_floor'] = house_floor
                item['house_year'] = house_year
                item['house_type'] = house_type
                # 是否近地铁
                item['is_near_subway'] = '近地铁' in clean_tags
                # 是否随时看房
                item['is_anytime']= '随时看房' in clean_tags
                # 免税类型
                tax_tag=[tag for tag in clean_tags if '房本满五年' in tag or '房本满两年' in tag ]
                item['tax_for_free'] = ','.join(tax_tag)


                # 输出Item
                yield item
        except Exception as e:
            self.logger.error(f"列表页解析异常:{e}")